{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection \n",
    "- First the taffic data need to be download from the Madrid Open Data Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "file_path = '/home/dice/leyuan/leyuan_p2/data/traffic_data/10-2023.csv'\n",
    "\n",
    "traffic_oct_2023 = pd.read_csv(file_path, sep=';')\n",
    "traffic_oct_2023[['date', 'time']] = traffic_oct_2023['fecha'].str.split(' ', expand=True)\n",
    "traffic_oct_2023.drop('fecha', axis=1, inplace=True)\n",
    "\n",
    "# extract all sensors' id in R19\n",
    "geo_19 = [\n",
    "    {\"id\": 3695, \"longitud\": -3.60483, \"latitud\": 40.404},\n",
    "    {\"id\": 3696, \"longitud\": -3.60923, \"latitud\": 40.4056},\n",
    "    {\"id\": 3752, \"longitud\": -3.61649, \"latitud\": 40.4088},\n",
    "    {\"id\": 3782, \"longitud\": -3.60963, \"latitud\": 40.4085},\n",
    "    {\"id\": 3783, \"longitud\": -3.60996, \"latitud\": 40.4108},\n",
    "    {\"id\": 6194, \"longitud\": -3.61612, \"latitud\": 40.4069},\n",
    "    {\"id\": 6195, \"longitud\": -3.61469, \"latitud\": 40.4075},\n",
    "    {\"id\": 6196, \"longitud\": -3.6147, \"latitud\": 40.4081},\n",
    "    {\"id\": 6197, \"longitud\": -3.61421, \"latitud\": 40.4087},\n",
    "    {\"id\": 6198, \"longitud\": -3.6138, \"latitud\": 40.407},\n",
    "    {\"id\": 6199, \"longitud\": -3.6128, \"latitud\": 40.4074},\n",
    "    {\"id\": 6200, \"longitud\": -3.61128, \"latitud\": 40.4051},\n",
    "    {\"id\": 6201, \"longitud\": -3.61123, \"latitud\": 40.405},\n",
    "    {\"id\": 6202, \"longitud\": -3.61077, \"latitud\": 40.4042},\n",
    "    {\"id\": 6203, \"longitud\": -3.60731, \"latitud\": 40.4041},\n",
    "    {\"id\": 6204, \"longitud\": -3.60919, \"latitud\": 40.4062},\n",
    "    {\"id\": 6205, \"longitud\": -3.60942, \"latitud\": 40.4083},\n",
    "    {\"id\": 6206, \"longitud\": -3.60833, \"latitud\": 40.4091},\n",
    "    {\"id\": 6207, \"longitud\": -3.60454, \"latitud\": 40.4076},\n",
    "    {\"id\": 6208, \"longitud\": -3.60448, \"latitud\": 40.4073},\n",
    "    {\"id\": 6209, \"longitud\": -3.61237, \"latitud\": 40.4093},\n",
    "    {\"id\": 6210, \"longitud\": -3.61286, \"latitud\": 40.4092},\n",
    "    {\"id\": 6211, \"longitud\": -3.61497, \"latitud\": 40.4056},\n",
    "    {\"id\": 6212, \"longitud\": -3.61623, \"latitud\": 40.4053},\n",
    "    {\"id\": 6213, \"longitud\": -3.6107, \"latitud\": 40.4077},\n",
    "    {\"id\": 6214, \"longitud\": -3.5966, \"latitud\": 40.4041},\n",
    "    {\"id\": 6215, \"longitud\": -3.59395, \"latitud\": 40.4027},\n",
    "    {\"id\": 6216, \"longitud\": -3.59417, \"latitud\": 40.404},\n",
    "    {\"id\": 6217, \"longitud\": -3.59606, \"latitud\": 40.4026},\n",
    "    {\"id\": 6218, \"longitud\": -3.61623, \"latitud\": 40.4087},\n",
    "    {\"id\": 6219, \"longitud\": -3.61136, \"latitud\": 40.4094},\n",
    "    {\"id\": 6220, \"longitud\": -3.6091, \"latitud\": 40.4076},\n",
    "    {\"id\": 6506, \"longitud\": -3.62178, \"latitud\": 40.3912},\n",
    "    {\"id\": 6922, \"longitud\": -3.60975, \"latitud\": 40.4041},\n",
    "    {\"id\": 6923, \"longitud\": -3.60129, \"latitud\": 40.4039},\n",
    "    {\"id\": 6930, \"longitud\": -3.60673, \"latitud\": 40.4041},\n",
    "    {\"id\": 6984, \"longitud\": -3.61872, \"latitud\": 40.397},\n",
    "    {\"id\": 6985, \"longitud\": -3.6212, \"latitud\": 40.3976},\n",
    "    {\"id\": 6986, \"longitud\": -3.62105, \"latitud\": 40.3969},\n",
    "    {\"id\": 6987, \"longitud\": -3.62393, \"latitud\": 40.3982},\n",
    "    {\"id\": 6988, \"longitud\": -3.62391, \"latitud\": 40.3976},\n",
    "    {\"id\": 6989, \"longitud\": -3.6223, \"latitud\": 40.3986},\n",
    "    {\"id\": 6990, \"longitud\": -3.62275, \"latitud\": 40.3965},\n",
    "    {\"id\": 10400, \"longitud\": -3.59447, \"latitud\": 40.4039},\n",
    "    {\"id\": 10943, \"longitud\": -3.61491, \"latitud\": 40.4056}\n",
    "]\n",
    "\n",
    "region_19_ids = [entry[\"id\"] for entry in geo_19]\n",
    "\n",
    "# Filter the traffic_oct_2023 DataFrame to include only rows where 'id' is in geo_19\n",
    "traffic_oct_2023_r19 = traffic_oct_2023[traffic_oct_2023['id'].isin(region_19_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'date' column\n",
    "grouped_traffic = traffic_oct_2023_r19.groupby('date')\n",
    "\n",
    "# 2023-10-02\n",
    "traffic_10_02_R19 = grouped_traffic.get_group('2023-10-02')\n",
    "# 2023-10-03\n",
    "traffic_10_03_R19 = grouped_traffic.get_group('2023-10-03')\n",
    "# 2023-10-04\n",
    "traffic_10_04_R19 = grouped_traffic.get_group('2023-10-04')\n",
    "# 2023-10-05\n",
    "traffic_10_05_R19 = grouped_traffic.get_group('2023-10-05')\n",
    "# 2023-10-06\n",
    "traffic_10_06_R19 = grouped_traffic.get_group('2023-10-06')\n",
    "# 2023-10-07\n",
    "traffic_10_07_R19 = grouped_traffic.get_group('2023-10-07')\n",
    "# 2023-10-08\n",
    "traffic_10_08_R19 = grouped_traffic.get_group('2023-10-08')\n",
    "\n",
    "\n",
    "# get traffic for the work days of the first week 10-02 10-06\n",
    "# Create a list of DataFrames to concatenate\n",
    "dfs_to_concat = [traffic_10_02_R19, traffic_10_03_R19, traffic_10_04_R19, traffic_10_05_R19, traffic_10_06_R19,traffic_10_07_R19, traffic_10_08_R19]\n",
    "\n",
    "# Concatenate the DataFrames along rows (axis=0)\n",
    "traffic_r19_w1 = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "# 2nd week\n",
    "# 2023-10-09\n",
    "traffic_10_09_R19 = grouped_traffic.get_group('2023-10-09')\n",
    "# 2023-10-10\n",
    "traffic_10_10_R19 = grouped_traffic.get_group('2023-10-10')\n",
    "# 2023-10-11\n",
    "traffic_10_11_R19 = grouped_traffic.get_group('2023-10-11')\n",
    "# 2023-10-12\n",
    "traffic_10_12_R19 = grouped_traffic.get_group('2023-10-12')\n",
    "# 2023-10-13\n",
    "traffic_10_13_R19 = grouped_traffic.get_group('2023-10-13')\n",
    "# 2023-10-14\n",
    "traffic_10_14_R19 = grouped_traffic.get_group('2023-10-14')\n",
    "\n",
    "# 2023-10-15\n",
    "traffic_10_15_R19 = grouped_traffic.get_group('2023-10-15')\n",
    "\n",
    "\n",
    "dfs2_to_concat = [traffic_10_09_R19, traffic_10_10_R19, traffic_10_11_R19, traffic_10_12_R19, traffic_10_13_R19,traffic_10_14_R19, traffic_10_15_R19]\n",
    "\n",
    "# Concatenate the DataFrames along rows (axis=0)\n",
    "traffic_r19_w2 = pd.concat(dfs2_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "# 3rd week\n",
    "# 2023-10-16\n",
    "traffic_10_16_R19 = grouped_traffic.get_group('2023-10-16')\n",
    "# 2023-10-17\n",
    "traffic_10_17_R19 = grouped_traffic.get_group('2023-10-17')\n",
    "# 2023-10-18\n",
    "traffic_10_18_R19 = grouped_traffic.get_group('2023-10-18')\n",
    "# 2023-10-19\n",
    "traffic_10_19_R19 = grouped_traffic.get_group('2023-10-19')\n",
    "# 2023-10-20\n",
    "traffic_10_20_R19 = grouped_traffic.get_group('2023-10-20')\n",
    "# 2023-10-21\n",
    "traffic_10_21_R19 = grouped_traffic.get_group('2023-10-21')\n",
    "# 2023-10-22\n",
    "traffic_10_22_R19 = grouped_traffic.get_group('2023-10-22')\n",
    "\n",
    "dfs3_to_concat = [traffic_10_16_R19, traffic_10_17_R19, traffic_10_18_R19, traffic_10_19_R19, traffic_10_20_R19, traffic_10_21_R19, traffic_10_22_R19]\n",
    "\n",
    "# Concatenate the DataFrames along rows (axis=0)\n",
    "traffic_r19_w3 = pd.concat(dfs3_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "#4th week\n",
    "# 2023-10-23\n",
    "traffic_10_23_R19 = grouped_traffic.get_group('2023-10-23')\n",
    "# 2023-10-24\n",
    "traffic_10_24_R19 = grouped_traffic.get_group('2023-10-24')\n",
    "# 2023-10-25\n",
    "traffic_10_25_R19 = grouped_traffic.get_group('2023-10-25')\n",
    "# 2023-10-26\n",
    "traffic_10_26_R19 = grouped_traffic.get_group('2023-10-26')\n",
    "# 2023-10-27\n",
    "traffic_10_27_R19 = grouped_traffic.get_group('2023-10-27')\n",
    "# 2023-10-28\n",
    "traffic_10_28_R19 = grouped_traffic.get_group('2023-10-28')\n",
    "# 2023-10-29\n",
    "traffic_10_29_R19 = grouped_traffic.get_group('2023-10-29')\n",
    "\n",
    "# 2023-10-30\n",
    "traffic_10_30_R19 = grouped_traffic.get_group('2023-10-30')\n",
    "# 2023-10-31\n",
    "traffic_10_31_R19 = grouped_traffic.get_group('2023-10-31')\n",
    "\n",
    "dfs4_to_concat = [traffic_10_23_R19, traffic_10_24_R19, traffic_10_25_R19, traffic_10_26_R19,traffic_10_27_R19,traffic_10_28_R19, traffic_10_29_R19,traffic_10_30_R19, traffic_10_31_R19]\n",
    "\n",
    "# Concatenate the DataFrames along rows (axis=0)\n",
    "traffic_r19_w4 = pd.concat(dfs4_to_concat, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs5_to_concat = [traffic_r19_w1, traffic_r19_w2, traffic_r19_w3, traffic_r19_w4]\n",
    "workdays_traffic = pd.concat(dfs5_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "# Export the code to local "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic data \n",
    "unique_ids = traffic_10_02_R19.keys()\n",
    "\n",
    "for id_to_visualize in unique_ids:\n",
    "    df_to_visualize = traffic_10_02_R19[id_to_visualize]\n",
    "    \n",
    "    df_to_visualize['datetime'] = pd.to_datetime(df_to_visualize['date'] + ' ' + df_to_visualize['time'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df_to_visualize['datetime'], df_to_visualize['intensidad'], label='Traffic')\n",
    "    plt.title(f'Traffic intensity (ID {id_to_visualize})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Traffic Intensity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized traffic data \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_ids = traffic_10_02_R19.keys()\n",
    "\n",
    "for id_to_visualize in unique_ids:\n",
    "    df_to_visualize = traffic_10_02_R19[id_to_visualize]\n",
    "    \n",
    "    df_to_visualize['datetime'] = pd.to_datetime(df_to_visualize['date'] + ' ' + df_to_visualize['time'])\n",
    "    \n",
    "    # Normalize traffic intensity values\n",
    "    max_intensity = df_to_visualize['intensidad'].max()\n",
    "    min_intensity = df_to_visualize['intensidad'].min()\n",
    "    df_to_visualize['normalized_intensity'] = (df_to_visualize['intensidad'] - min_intensity) / (max_intensity - min_intensity)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df_to_visualize['datetime'], df_to_visualize['normalized_intensity'], label='Normalized Traffic Intensity')\n",
    "    plt.title(f'Normalized Traffic intensity (ID {id_to_visualize})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Normalized Traffic Intensity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the traffic flows figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_folder = 'traffic_day1'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "unique_ids = traffic_10_02_R19.keys()\n",
    "\n",
    "for id_to_visualize in unique_ids:\n",
    "\n",
    "    df_to_visualize = traffic_10_02_R19[id_to_visualize]\n",
    "    \n",
    "    df_to_visualize['datetime'] = pd.to_datetime(df_to_visualize['date'] + ' ' + df_to_visualize['time'])\n",
    "    \n",
    "    # Normalize traffic intensity values\n",
    "    max_intensity = df_to_visualize['intensidad'].max()\n",
    "    min_intensity = df_to_visualize['intensidad'].min()\n",
    "    df_to_visualize['normalized_intensity'] = (df_to_visualize['intensidad'] - min_intensity) / (max_intensity - min_intensity)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df_to_visualize['datetime'], df_to_visualize['normalized_intensity'], label='Normalized Traffic Intensity')\n",
    "    plt.title(f'Normalized Traffic intensity (ID {id_to_visualize})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Normalized Traffic Intensity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    save_path = os.path.join(save_folder, f'traffic_intensity_ID_{id_to_visualize}.png')\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load images and compute histograms\n",
    "folder_path = '/home/dice/leyuan/leyuan_p2/data/traffic_data/traffic_day1'\n",
    "\n",
    "# Load and compute histograms for new graphs\n",
    "new_images = [cv2.imread(os.path.join(folder_path, file)) for file in os.listdir(folder_path)]\n",
    "new_histograms = [cv2.calcHist([image], [0], None, [256], [0, 256]).flatten() for image in new_images]\n",
    "\n",
    "# Normalize histograms\n",
    "new_histograms_normalized = [histogram / np.linalg.norm(histogram) for histogram in new_histograms]\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarities_cos = cosine_similarity(new_histograms_normalized)\n",
    "\n",
    "# Visualization of similarity matrix\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size\n",
    "plt.imshow(similarities_cos, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Cosine Similarity Matrix of New Traffic Intensity ')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Image Index')\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity + Deep Learning Model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Define a function to preprocess an image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def compute_embedding(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    embedding = model.predict(img)\n",
    "    return embedding.flatten()  # Flatten the embedding to a 1D vector\n",
    "\n",
    "# Directory \n",
    "image_dir = '/home/dice/leyuan/leyuan_p2/data/traffic_data/traffic_day1'\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Check if there are image files\n",
    "if not image_files:\n",
    "    print(\"No image files found in the directory.\")\n",
    "else:\n",
    "    # Compute embeddings for all images\n",
    "    embeddings = [compute_embedding(img_path) for img_path in image_files]\n",
    "\n",
    "    # Compute similarities between embeddings\n",
    "    similarity_matrix_DL = cosine_similarity(embeddings)\n",
    "\n",
    "    # Visualize similarity matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(similarity_matrix_DL, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title('Cosine Similarity between Image Embeddings for 1 day')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Image Index')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "similarity_threshold = 0.85\n",
    "\n",
    "sensor1_ids = []\n",
    "sensor2_ids = []\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i+1, len(embeddings)):\n",
    "        if similarity_matrix_DL[i][j] > similarity_threshold:\n",
    "            # Extract the IDs from the file paths\n",
    "            id_i = os.path.basename(image_files[i]).split('_')[4]  \n",
    "            id_j = os.path.basename(image_files[j]).split('_')[4]  \n",
    "\n",
    "            # Append the IDs and their similarity value to the lists\n",
    "            sensor1_ids.append(id_i)\n",
    "            sensor2_ids.append(id_j)\n",
    "            similarities.append(similarity_matrix_DL[i][j])\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "similarity_df = pd.DataFrame({'Sensor1': sensor1_ids, 'Sensor2': sensor2_ids, 'Similarity': similarities})\n",
    "\n",
    "# Define a dictionary to store similar sensors\n",
    "similar_sensors = {}\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in similarity_df.iterrows():\n",
    "    sensor1 = row['Sensor1']\n",
    "    sensor2 = row['Sensor2']\n",
    "    similarity = row['Similarity']\n",
    "    \n",
    "    # Check if Sensor1 exists in the dictionary\n",
    "    if sensor1 in similar_sensors:\n",
    "        similar_sensors[sensor1].append(sensor2)\n",
    "    else:\n",
    "        similar_sensors[sensor1] = [sensor2]\n",
    "    \n",
    "    # Check if Sensor2 exists in the dictionary\n",
    "    if sensor2 in similar_sensors:\n",
    "        similar_sensors[sensor2].append(sensor1)\n",
    "    else:\n",
    "        similar_sensors[sensor2] = [sensor1]\n",
    "\n",
    "# Define a function to remove duplicate values from a list\n",
    "def remove_duplicates(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "# Remove duplicate values from the similar_sensors dictionary\n",
    "for key in similar_sensors:\n",
    "    similar_sensors[key] = remove_duplicates(similar_sensors[key])\n",
    "\n",
    "# Output the merged results\n",
    "merged_results = []\n",
    "for key, value in similar_sensors.items():\n",
    "    if len(value) > 1:\n",
    "        merged_results.append({'Sensor1': key, 'Sensor2': value, 'Similarity': similarity_df[(similarity_df['Sensor1'] == key) & (similarity_df['Sensor2'].isin(value))]['Similarity'].mean()})\n",
    "\n",
    "# Convert merged results to DataFrame\n",
    "merged_results_df = pd.DataFrame(merged_results)\n",
    "\n",
    "print(merged_results_df)\n",
    "\n",
    "# Save the merged results DataFrame to a CSV file\n",
    "merged_results_df.to_csv('similar_sensors_DL_day.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_day = pd.read_csv('similar_sensors_DL_day.csv')\n",
    "df_month = pd.read_csv('similar_sensors_DL_month.csv')\n",
    "df_week = pd.read_csv('similar_sensors_week.csv')\n",
    "\n",
    "\n",
    "df_day['Sensor2'] = df_day['Sensor2'].apply(lambda x: eval(x))\n",
    "df_month['Sensor2'] = df_month['Sensor2'].apply(lambda x: eval(x))\n",
    "df_week['Sensor2'] = df_week['Sensor2'].apply(lambda x: eval(x))\n",
    "\n",
    "dataframes = [df_day, df_month, df_week]  \n",
    "\n",
    "common_sensors = []\n",
    "\n",
    "for index, row in df_day.iterrows():\n",
    "    sensor1 = row['Sensor1']\n",
    "    sensor2_sets = [set(row['Sensor2'])]  \n",
    "    \n",
    "    for df in dataframes[1:]: \n",
    "        row_df = df[df['Sensor1'] == sensor1]\n",
    "        if not row_df.empty:\n",
    "            sensor2_sets.append(set(row_df.iloc[0]['Sensor2']))\n",
    "    \n",
    "    common_set = set.intersection(*sensor2_sets) if sensor2_sets else set()\n",
    "    \n",
    "    if common_set:\n",
    "        similarities = [row['Similarity']]\n",
    "        for df in dataframes[1:]:\n",
    "            row_df = df[df['Sensor1'] == sensor1]\n",
    "            if not row_df.empty:\n",
    "                similarities.append(row_df.iloc[0]['Similarity'])\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        common_sensors.append({'Sensor1': sensor1, 'CommonSensor2': list(common_set), 'AvgSimilarity': avg_similarity})\n",
    "\n",
    "common_sensors_df = pd.DataFrame(common_sensors)\n",
    "\n",
    "print(common_sensors_df.to_csv(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for index, row in common_sensors_df.iterrows():\n",
    "    sensor1 = str(row['Sensor1']) \n",
    "    G.add_node(sensor1)\n",
    "    for sensor in row['CommonSensor2']:\n",
    "        sensor2 = str(sensor) \n",
    "        G.add_node(sensor2)  \n",
    "        G.add_edge(sensor1, sensor2)\n",
    "\n",
    "\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "node_sizes = [v * 150 for v in degrees.values()]  \n",
    "\n",
    "# color & size\n",
    "max_degree = max(degrees.values())\n",
    "colors = [(degree / max_degree) for degree in degrees.values()]  \n",
    "node_colors = [plt.cm.Blues(color) for color in colors]  \n",
    "\n",
    "plt.figure(figsize=(8, 8))  \n",
    "nx.draw(G, pos, with_labels=True, edge_color='k', node_size=node_sizes, node_color=node_colors, font_size=8, width=1, alpha=0.7)\n",
    "\n",
    "plt.title('Network Graph of Similar Sensors for District 19', size=15)\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "def train_lstm_model(data):\n",
    "    models = {} \n",
    "    for sensor_id, df in data.items():\n",
    "        X = df['intensidad'].values.reshape(-1, 1)\n",
    "        y = df['intensidad'].shift(-1).fillna(method='ffill').values.reshape(-1, 1)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, activation='relu', input_shape=(1, 1)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        \n",
    "        model.fit(X_train.reshape(-1, 1, 1), y_train, epochs=100, batch_size=32, verbose=0)\n",
    "        \n",
    "        models[sensor_id] = model\n",
    "        \n",
    "        loss = model.evaluate(X_test.reshape(-1, 1, 1), y_test, verbose=0)\n",
    "        print(f'Model for sensor {sensor_id}: Loss={loss}')\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_traffic_data(models, sensor_id, data_point):\n",
    "    model = models.get(sensor_id)\n",
    "    if model:\n",
    "        prediction = model.predict(np.array([data_point]).reshape(1, 1, 1))\n",
    "        print(f'Predicted traffic data for sensor {sensor_id}: {prediction}')\n",
    "    else:\n",
    "        print(f'Model for sensor {sensor_id} not found.')\n",
    "\n",
    "workdays_df = {\n",
    "    3695: traffic_3695_oct,  # example\n",
    "    # more traffic data \n",
    "}\n",
    "\n",
    "trained_models = train_lstm_model(workdays_df)\n",
    "\n",
    "# prediction \n",
    "sensor_id_to_predict = 3695  # ID of traffic sensor to predict\n",
    "data_point_to_predict = 82  \n",
    "predict_traffic_data(trained_models, sensor_id_to_predict, data_point_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the real Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folium\n",
    "\n",
    "mymap = folium.Map(location=[40.4168, -3.7038], zoom_start=12)\n",
    "\n",
    "# Sensor locations \n",
    "sensor_locations = {\n",
    "    3695: (40.404, -3.60483),\n",
    "    3696: (40.4056, -3.60923),\n",
    "    3752: (40.4088, -3.61649),\n",
    "    3782: (40.4085, -3.60963),\n",
    "    3783: (40.4108, -3.60996),\n",
    "    6194: (40.4069, -3.61612),\n",
    "    6195: (40.4075, -3.61469),\n",
    "    6196: (40.4081, -3.6147),\n",
    "    6197: (40.4087, -3.61421),\n",
    "    6198: (40.407, -3.6138),\n",
    "    6199: (40.4074, -3.6128),\n",
    "    6200: (40.4051, -3.61128),\n",
    "    6201: (40.405, -3.61123),\n",
    "    6202: (40.4042, -3.61077),\n",
    "    6203: (40.4041, -3.60731),\n",
    "    6204: (40.4062, -3.60919),\n",
    "    6205: (40.4083, -3.60942),\n",
    "    6206: (40.4091, -3.60833),\n",
    "    6207: (40.4076, -3.60454),\n",
    "    6208: (40.4073, -3.60448),\n",
    "    6209: (40.4093, -3.61237),\n",
    "    6210: (40.4092, -3.61286),\n",
    "    6211: (40.4056, -3.61497),\n",
    "    6212: (40.4053, -3.61623),\n",
    "    6213: (40.4077, -3.6107),\n",
    "    6214: (40.4041, -3.5966),\n",
    "    6215: (40.4027, -3.59395),\n",
    "    6216: (40.404, -3.59417),\n",
    "    6217: (40.4026, -3.59606),\n",
    "    6218: (40.4087, -3.61623),\n",
    "    6219: (40.4094, -3.61136),\n",
    "    6220: (40.4076, -3.6091),\n",
    "    6506: (40.3912, -3.62178),\n",
    "    6922: (40.4041, -3.60975),\n",
    "    6923: (40.4039, -3.60129),\n",
    "    6930: (40.4041, -3.60673),\n",
    "    6984: (40.397, -3.61872),\n",
    "    6985: (40.3976, -3.6212),\n",
    "    6986: (40.3969, -3.62105),\n",
    "    6987: (40.3982, -3.62393),\n",
    "    6988: (40.3976, -3.62391),\n",
    "    6989: (40.3986, -3.6223),\n",
    "    6990: (40.3965, -3.62275),\n",
    "    10400: (40.4039, -3.59447),\n",
    "    10943: (40.4056, -3.61491)\n",
    "}\n",
    "\n",
    "# Connections between sensors\n",
    "connections = [\n",
    "    (6203, [6922, 6195]),\n",
    "    (6930, [6923]),\n",
    "    (6202, [6212, 6207, 6198, 6205]),\n",
    "    (6210, [6198, 6205, 6196, 6214, 6207]),\n",
    "    (6922, [6204, 6194, 6203, 6212, 3783, 3782, 6506, 6195, 6923, 6207, 6200]),\n",
    "    (6216, [6194, 6198, 6212, 6205, 6214, 3696]),\n",
    "    (6200, [6204, 6206, 3783, 6922, 6506, 6195]),\n",
    "    (6209, [6206, 6212, 3783, 6506, 6197, 6208]),\n",
    "    (6196, [6194, 6198, 6205, 6210, 6214, 6207]),\n",
    "    (6208, [6209, 6206, 3783, 3782, 6197, 6506, 6214, 6195, 6923]),\n",
    "    (6204, [6194, 6206, 3783, 6922, 6205, 6214, 6195, 6923, 6207, 6200]),\n",
    "    (6211, [10943, 6213]),\n",
    "    (6198, [6205, 6196, 6210, 6202, 6195, 3696, 6207]),\n",
    "    (6206, [6204, 6209, 3783, 3782, 6197, 6506, 6214, 6195, 6923, 6207, 6208, 6200]),\n",
    "    (6212, [6922, 6202, 6209, 3782]),\n",
    "    (6506, [6194, 6209, 6206, 3783, 6922, 6197, 6195, 6923, 6208, 6200]),\n",
    "    (6214, [6194, 6204, 6206, 10400, 6205, 6196, 6210, 6217, 6207, 6208, 6923]),\n",
    "    (6207, [6204, 6194, 6206, 6198, 6922, 6205, 6210, 6214, 6196, 6202, 6923]),\n",
    "    (6205, [6204, 6194, 6198, 6210, 6214, 6196, 6202, 6207]),\n",
    "    (6194, [6204, 3783, 3782, 6922, 6205, 6506, 6196, 6214, 6217, 6207, 6923]),\n",
    "    (6923, [6204, 6194, 6206, 6930, 6922, 6506, 6197, 6214, 6195, 6207, 6208]),\n",
    "    (3783, [6204, 6194, 6209, 6206, 3782, 6922, 6506, 6197, 6195, 6208, 6200]),\n",
    "    (6195, [6204, 6206, 6198, 6203, 3783, 3782, 6922, 6506, 6923, 6208, 6200]),\n",
    "    (6197, [6209, 6206, 3783, 6506, 6208, 6923]),\n",
    "    (3695, [6213, 3696]),\n",
    "    (3782, [6194, 6206, 6212, 3783, 6922, 6195, 3696, 6208]),\n",
    "    (6217, [6194, 6214, 10400]),\n",
    "    (10400, [6217, 6214]),\n",
    "    (6213, [10943, 6211, 3695]),\n",
    "    (10943, [6213, 6211]),\n",
    "    (3696, [3695, 6198, 3782]),\n",
    "    (6219, [3696])\n",
    "]\n",
    "\n",
    "# Calculate the number of connections for each sensor\n",
    "sensor_connections = {}\n",
    "for connection in connections:\n",
    "    src_sensor_id = connection[0]\n",
    "    connected_sensor_ids = connection[1]\n",
    "    for sensor_id in connected_sensor_ids:\n",
    "        sensor_connections[sensor_id] = sensor_connections.get(sensor_id, 0) + 1\n",
    "\n",
    "# Add markers for sensor locations\n",
    "for sensor_id, (lat, lon) in sensor_locations.items():\n",
    "    size = 8 + sensor_connections.get(sensor_id, 0) * 2 \n",
    "    color = '#3186cc' if sensor_connections.get(sensor_id, 0) > 1 else '#FF5733'\n",
    "    folium.CircleMarker(location=[lat, lon], radius=size, color=color, fill=True, fill_color=color,\n",
    "                        popup=f\"Sensor ID: {sensor_id}\",  \n",
    "                        fill_opacity=0.6).add_to(mymap)  \n",
    "\n",
    "# Add connections\n",
    "for src, dest_list in connections:\n",
    "    src_lat, src_lon = sensor_locations[src]\n",
    "    for dest in dest_list:\n",
    "        dest_lat, dest_lon = sensor_locations[dest]\n",
    "        folium.PolyLine(locations=[(src_lat, src_lon), (dest_lat, dest_lon)], color='#3388ff', weight=2, opacity=0.7).add_to(mymap)\n",
    "\n",
    "# Save and display\n",
    "mymap.save(\"sensor_connections_map.html\")\n",
    "mymap"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
